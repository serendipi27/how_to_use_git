{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('C:/pytest/chatdata_small.csv', encoding='cp949')\n",
    "text = '가고싶은 학교 떨어졌어'\n",
    "text1 = '1지망 학교 떨어졌어'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file = 'C:/pytest/data/model_name/transformer.pickle', mode='rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "    \n",
    "word_index = loaded_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'transformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index.update({\"SOS\": 1, \"EOS\": 2}) #질문\n",
    "\n",
    "char2idx_dict = word_index\n",
    "idx2char_dict = {y: x for x, y in word_index.items()}\n",
    "\n",
    "char2idx_dict['<PAD>']=0\n",
    "char2idx_dict['<SOS>']= char2idx_dict['SOS']\n",
    "del char2idx_dict['SOS']\n",
    "\n",
    "char2idx_dict['<END>'] = char2idx_dict['EOS']\n",
    "del char2idx_dict['EOS']\n",
    "\n",
    "idx2char_dict[0] = '<PAD>'\n",
    "idx2char_dict[1] = '<SOS>'\n",
    "idx2char_dict[2] = '<END>'  #질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_configs = dict({'char2idx':char2idx_dict, 'idx2char':idx2char_dict, \n",
    "                       'vocab_size':len(word_index), 'pad_symbol': '<PAD>', \n",
    "                       'std_symbol': '<SOS>', 'end_symbol': '<END>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_INDEX = 1\n",
    "char2idx = prepro_configs['char2idx']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "MAX_SEQUENCE = 25\n",
    "\n",
    "kargs = {'model_name': model_name,\n",
    "         'num_layers': 2,\n",
    "         'd_model': 512,\n",
    "         'num_heads': 8,\n",
    "         'dff': 2048,\n",
    "         'input_vocab_size':vocab_size,\n",
    "         'target_vocab_size': vocab_size,\n",
    "         'maximum_position_encoding': MAX_SEQUENCE,\n",
    "         'end_token_idx': char2idx[end_index],\n",
    "         'rate': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스킹\n",
    "def create_padding_mask(seq):\n",
    "    # 시퀀스가 0과 같으면 1을 출력해라  #cast는 float32형으로만 바꿔주는 기능\n",
    "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)  \n",
    "    # 1차원을 3차원으로 만들어줌\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]   #3차원으로 변환\n",
    "\n",
    "# 상삼각행렬생성(대각선제외)\n",
    "def create_look_ahead_mask(size):\n",
    "    #band_part(x,y,z)인데 z를 -1로 하면 대각선포함 윗부분이 1이됨\n",
    "    #y를 -1로 하면 대각선포함 아랫쪽만 1로 만듬\n",
    "    # x는 2차원 이상 행렬이 들어와야함, output의 형상\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    # 1에서 대각선포함아랫쪽1을 빼면 대각선제외 위쪽1만 남음\n",
    "    return mask\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)      # 인코더 패딩 마스크\n",
    "    dec_padding_mask = create_padding_mask(inp) \n",
    "    #인코딩,디코딩 패딩 마스크 생성\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                            np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q,k, transpose_b=True)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#멀티헤드어텐션, 피드포워드네트워크, 인/디코더레이어, 인/디코더\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):                             # 초기화 함수\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = kargs['num_heads']\n",
    "        self.d_model = kargs['d_model']\n",
    "\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        # 셀프어텐션 결과 출력\n",
    "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
    "\n",
    "    # 각 배치마다 (seq_len * depth) 형태를 (num_heads * seq_len * deqth)로 변환\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        # reshape 할때 -1을 넣으면 이후 기입된 부분의 축변환을 하고난 뒤 남은 값을 자동 배정\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "def feed_forward_network(**kargs):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(kargs['dff'],activation='relu'),\n",
    "        tf.keras.layers.Dense(kargs['d_model'])\n",
    "        ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):                               # 초기화\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(**kargs)                 # 멀티 헤드 어텐션 레이어 생성\n",
    "        self.ffn = feed_forward_network(**kargs)     #파워포워드네트워크\n",
    "\n",
    "        #층정규화\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)              \n",
    "        ffn_output = self.dropout2(ffn_output)   \n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "# 디코더레이어 준비\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(**kargs)      # 멀티 헤드 어텐션 레이어 1\n",
    "        self.mha2 = MultiHeadAttention(**kargs)      # 멀티 헤드 어텐션 레이어 2\n",
    "        self.ffn = feed_forward_network(**kargs)     # 포지션 와이즈 피드 포워드 네트워크\n",
    "\n",
    "        # 층 정규화\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout Layer\n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)                    # 멀티 헤드 어텐션 레이어 1 수행\n",
    "        attn1 = self.dropout1(attn1)              # 드롭아웃 수행\n",
    "        out1 = self.layernorm1(attn1 + x)         # 층 정규화 및 리지듀얼 커넥션 수행\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # 멀티 헤드 어텐션 레이어 2 수행\n",
    "        attn2 = self.dropout2(attn2)              # 드롭아웃 수행\n",
    "        out2 = self.layernorm2(attn2 + out1)      # 층 정규화 및 리지듀얼 커넥션 수행\n",
    "\n",
    "        ffn_output = self.ffn(out2)               # out2에 대해 피드포워드 연산 수행\n",
    "        ffn_output = self.dropout3(ffn_output)    # 드롭아웃 수행\n",
    "        out3 = self.layernorm3(ffn_output + out2) # 층 정규화 및 리지듀얼 커넥션 수행\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "\n",
    "        #워드 임베딩 레이어 생성\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=kargs['input_vocab_size'],\n",
    "                                                   output_dim = self.d_model)\n",
    "\n",
    "        #포지셔널 인코딩레이어 생성\n",
    "        self.pos_encoding = positional_encoding(position=kargs['maximum_position_encoding'],\n",
    "                                                d_model=self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len,:]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "\n",
    "        #워드 임베딩 레이어 생성(1)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=kargs['target_vocab_size'],\n",
    "                                                   output_dim = self.d_model)\n",
    "        # 포지셔널 인코딩 레이어 생성(2)\n",
    "        self.pos_encoding = positional_encoding(position = kargs['maximum_position_encoding'],\n",
    "                                                d_model = self.d_model)\n",
    "        # 디코더 레이어 생성,num_layers 수만큼 리스트 배열로 만든다.\n",
    "        self.dec_layers = [DecoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}   #딕셔너리초기화가 무슨말이지 그냥 처음 선언된 딕셔너리니까 빈값 생성해준거 아닌가 앞으로 추가할꺼니까\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:,:seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2, = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_bolck1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Transformer, self).__init__(name=kargs['model_name'])\n",
    "        self.end_token_idx = kargs['end_token_idx']\n",
    "        self.encoder = Encoder(**kargs)\n",
    "        self.decoder = Decoder(**kargs)\n",
    "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
    "              \n",
    "    def call(self, x):\n",
    "        inp, tar = x\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        \n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "        \n",
    "        dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def inference(self, x):\n",
    "        inp = x\n",
    "        tar = tf.expand_dims([STD_INDEX], axis=0)\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "\n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "            final_output = self.final_layer(dec_output)\n",
    "            outputs = tf.argmax(final_output, axis=-1).numpy()\n",
    "            pred_token = outputs[0][-1]\n",
    "            if pred_token == self.end_token_idx:\n",
    "                break\n",
    "            predict_tokens.append(pred_token)\n",
    "            tar = tf.expand_dims([STD_INDEX]+predict_tokens, axis=0)\n",
    "            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        return predict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수, 정확도함수\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss(real,pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(real,pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis = -1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real,pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델구성, 컴파일\n",
    "loaded_model = Transformer(**kargs)\n",
    "loaded_model.compile(optimizer = tf.keras.optimizers.Adam(1e-4), loss=loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pad = loaded_tokenizer.texts_to_sequences([text1])\n",
    "\n",
    "sample_input_pad = np.array([[34, 35, 36, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 2.9555 - accuracy: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 0s/step - loss: 2.9555 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c170b0d48>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.fit([sample_input_pad, sample_input_pad], sample_input_pad, batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights('C:/pytest/data/transformer/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def enc_processing(value,dictionary):\n",
    "    FILTERS = \"([~.,!?\\\"':;)(])\" # 정규화를 사용하여 필터에 들어 있는 값들을 \"\" 으로 치환 한다.\n",
    "    CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "    sequences_input_index = [] # 인덱스 값들을 가지고 있는 배열\n",
    "\n",
    "    for sequence in value: # 한 줄 씩 불러온다\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = [] # 하나의 문장을 인코딩 할 때 가지고 있기 위한 배열\n",
    "        for word in sequence.split(): # 문장을 스페이스 단위로 자른다\n",
    "            if dictionary.get(word) is not None: # 잘려진 단어들이 딕셔너리에 있으면 그 값을 sequence_index에 추가\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "        if len(sequence_index) > MAX_SEQUENCE: # 문장 제한 길이보다 길어질 경우 뒷 토큰을 자른다\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[\"<PAD>\"]]\n",
    "        sequences_input_index.append(sequence_index) # 인덱스화 되어 있는 값을 sequences_input_index에 넣어준다\n",
    "    return np.asarray(sequences_input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"3박4일~ 정도 놀러가고 싶다.\"\n",
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']                          \n",
    "test_index_inputs = enc_processing([text], char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여행은 언제나 좋죠\n"
     ]
    }
   ],
   "source": [
    "outputs = loaded_model.inference(test_index_inputs)\n",
    "print(' '.join([idx2char[output] for output in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 37,  7,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
